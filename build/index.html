<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <meta name="description" content="CodeIF: Benchmarking the Instruction-Following Capabilities of Large Language Models for Code Generation" />
  <meta name="keywords" content="CodeIF, Code, Large Language Models, LLM, Code LLM, Evaluation, Benchmark" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>
    CodeIF: Benchmarking the Instruction-Following Capabilities of Large Language Models for Code Generation
  </title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || []

      function gtag() {
        dataLayer.push(arguments)
      }

      gtag("js", new Date())

      gtag("config", "G-PYVRSFMDRL")
    </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="./css/bulma.min.css" />
  <link rel="stylesheet" href="./css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="./css/bulma-slider.min.css" />
  <link rel="stylesheet" href="./css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="./css/index.css" />
  <link rel="icon" href="./images/favicon.svg" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./js/fontawesome.all.min.js"></script>
  <script src="./js/bulma-carousel.min.js"></script>
  <script src="./js/bulma-slider.min.js"></script>
  <script src="./js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              CodeIF: Benchmarking the Instruction-Following Capabilities of Large Language Models for Code Generation
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://github.com/lin-rany/codeIF">Kaiwen Yan</a><sup>1 *</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=eynbo4cAAAAJ&hl=zh-CN">Hongcheng Guo</a><sup>1 * †</sup>,</span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Xuanqing_Shi1">Xuanqing Shi</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://github.com/lin-rany/codeIF">Jingyi Xu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://github.com/lin-rany/codeIF">Yaonan Gu</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Zhoujun_Li1">Zhoujun Li</a><sup>1</sup>,</span>

            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Beihang University</span>
              <span class="author-block" style="margin-left: 1em;"><sup>2</sup>Tsinghua University</span>
              <span class="author-block" style="margin-left: 1em;"><sup>3</sup>National University of Singapore</span>
            </div>
            <!-- 新增脚注 -->
            <div class="is-size-6 publication-footnotes" style="margin-top: 0.5rem;">
              <span class="footnote"><sup>*</sup> Equal contribution.</span>
              <span class="footnote" style="margin-left: 1em;"><sup>†</sup> Corresponding Author.</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2502.19166" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/lin-rany/codeIF"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/linrany/CodeIF"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
                <!-- Leaderboard Link. -->
                <span class="link-block">
                  <a href="leaderboard.html" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-trophy"></i>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered">
          <img src="./images/LCB_holistic_tasks.png" alt="Teaser" class="teaser-image center" width="80%" />
        </div>

        <h2 class="subtitle has-text-centered">
          <span class="dnerf">CodeIF</span> collects problems from periodic contests on <span
            class="dnerf">CodeIF</span>, <span class="dnerf">AtCoder</span>, and <span class="dnerf">Codeforces</span>
          platforms and uses them for constructing a holistic benchmark for evaluating Code LLMs across variety of
          code-related scenarios continuously over time.
        </h2>

      </div>
    </div>
  </section> -->


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              With the rapid advancement of Large Language Models (LLMs), the demand for robust instruction-following capabilities in code generation tasks has grown significantly. Code generation not only facilitates faster prototyping and automated testing, but also augments developer efficiency through improved maintainability and reusability of code. In this paper, we introduce CodeIF, the first benchmark specifically designed to assess the abilities of LLMs to adhere to task-oriented instructions within diverse code generation scenarios. CodeIF encompasses a broad range of tasks, including function synthesis, error debugging, algorithmic refactoring, and code explanation, thereby providing a comprehensive suite to evaluate model performance across varying complexity levels and programming domains. We conduct extensive experiments with LLMs, analyzing their strengths and limitations in meeting the demands of these tasks. The experimental results offer valuable insights into how well current models align with human instructions, as well as the extent to which they can generate consistent, maintainable, and contextually relevant code. Our findings not only underscore the critical role that instruction-following LLMs can play in modern software development, but also illuminate pathways for future research aimed at enhancing their adaptability, reliability, and overall effectiveness in automated code generation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">DataSet</h2>
          <div class="content has-text-justified">
            <p>
              <span class="dnerf">CodeIF</span> annotates problems with release dates, and thus allows evaluating
              models on problems released during a specific time period. Thus, for a newer model
              with a training-cutoff date <span class="dnerf">D</span>, we can evaluate it on problems released after
              <span class="dnerf">D</span> to measure its generalization on <i>unseen</i> problems.
            </p>
          
            <div class="columns is-centered">
              <img src="./images/contamination1.png" alt="Code Generation Live Evaluation" class="teaser-image"
                width="48%" class="center" />
              <img src="./images/contamination2.png" alt="Test Output Prediction Live Evaluation" class="teaser-image"
                width="48%" class="center" />
            </div>

            <p>
              The above plots depict the performance of models on code generation and test output prediction scenarios
              on problems released over different months. We find that <span class="dnerf">DeepSeek</span> models
              exhibit a stark drop in performance on LeetCode problems released since September 2023, its release date,
              indicating that the earlier problems might be contaminated. In contrast, for
              <span class="dnerf">GPT</span>
              models, the performance is relatively stable across different months.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Holistic Evaluation and Open vs Closed Models</h2>
          <div class="content has-text-justified">
            <p>
              <span class="dnerf">LiveCodeBench</span> evaluates models on a variety of code-related scenarios, such as
              code generation, self-repair, test output prediction, and code execution. We find that while model
              performances
              are correlated across different scenarios, there relative performances and ordering can vary (left
              figure).
              For instance, <span class="dnerf">Claude-3-Opus</span> overtakes <span class="dnerf">GPT-4-turbo</span> in
              the
              test output prediction scenario, but not in the code generation scenario. Similarly,
              <span class="dnerf">Mistral-Large</span>
              performs considerably better on natural language reasoning tasks like test output prediction and code
              execution.
            </p>
            <br />
            <div class="columns is-centered">
              <img src="./images/tasks_radar.png" alt="Holistic Evaluation" class="teaser-image" width="44%"
                height="44%" class="center" />
              <img src="./images/lc_barchart.png" alt="Open vesus Closed Models" class="teaser-image" width="48%"
                class="center" />
            </div>
            <p>
              We compare the performance of open access models with closed api-access models on <span
                class="dnerf">LiveCodeBench</span> and find that generally the closed api-access models outperform the
              open models. Particularly, the only open models that surpass the barrier are fine-tuned variants of large
              (30+B parameter) models.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section> -->

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Pipeline</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/code_pipeline.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              The CodeIF framework, illustrated in Figure, is meticulously crafted to enhance code generation by leveraging constraint instructions derived from real-world coding tasks. This process involves a methodical collection and refinement of constraints, integral for ensuring the relevance and applicability to practical scenarios.

              By integrating these constraint instructions with advanced language models (LLMs) and a rigorous human review process, CodeIF succeeds in constructing a robust and high-quality evaluation dataset. This dataset not only benchmarks the performance of LLMs in generating code but also fosters the development of more intelligent, understanding, and contextually aware coding assistants. The assembly of the dataset follows a structured protocol, ensuring the inclusion of diverse programming tasks and scenarios, thereby broadening the framework's applicability and effectiveness across various coding environments and challenges.
            </p>
          </div>
        </div>
      </div>

    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Dataset</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/instruction_analysis.png" alt="Instruction Analysis" class="teaser-image"
                width="48%" class="center" />
              <img src="./images/data_set_instruct_type.png" alt="Instruct Type Analysis" class="teaser-image"
                width="48%" class="center" />
            </div>
            <!-- <div class="columns is-centered">
              <img src="./images/testgen_performance.png" alt="Code Execution Performance" class="teaser-image"
                width="48%" class="center" />
              <img src="./images/execution_performance.png" alt="Code Execution Performance" class="teaser-image"
                width="48%" class="center" />
            </div> -->
            <p>
              The CodeIF framework is meticulously designed to enhance automatic code generation capabilities by leveraging real-world constraints. It divides challenges into different levels of complexity and covers multiple programming languages to ensure comprehensive evaluation.

              Extensive Coverage: Supports Go, Python, Java, and C++, with task designs closely aligned with real-world coding scenarios.
              Structured Complexity: Provides datasets categorized into simple and difficult levels, aiding in targeted evaluation of language models under varying complexities.
              Detailed Insights: Offers in-depth insights into model performance across various programming instructions, helping identify areas for improvement.
              Explore the powerful capabilities of CodeIF in improving the precision and effectiveness of code generation models.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Submitting Custom Models</h2>
          <div class="content has-text-justified">
            <p>
              To submit models you can create a pull request on our <a
                href="https://github.com/LiveCodeBench/submissions">Github</a>. Particularly, you can copy your model
              generations folder from `output` to the `submissions` folder and create a pull request. We will review the
              submission and add the model to the leaderboard accordingly.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section> -->




  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@misc{yan2025codeifbenchmarkinginstructionfollowingcapabilities,
  title={CodeIF: Benchmarking the Instruction-Following Capabilities of Large Language Models for Code Generation}, 
  author={Kaiwen Yan and Hongcheng Guo and Xuanqing Shi and Jingyi Xu and Yaonan Gu and Zhoujun Li},
  year={2025},
  eprint={2502.19166},
  archivePrefix={arXiv},
  primaryClass={cs.SE},
  url={https://arxiv.org/abs/2502.19166}, 
}
      </code></pre>
    </div>
  </section>

  <!-- <footer class="footer">
    <div class="container">

      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Please reach out to <a href="mailto:naman_jain@berkeley.edu">naman_jain@berkeley.edu</a> for questions or
              feedback on LiveCodeBench. We are also open to collaborations and suggestions for new scenarios to add to
              the benchmark. Finally, LiveCodeBench provides one axis of LLM coding evaluations and we recommend the
              following leaderboards for measuring code LM ability on various coding tasks, such as
              <a href="https://evalplus.github.io/leaderboard.html">EvalPlus Leaderboard</a>,
              <a href="https://crux-eval.github.io/leaderboard.html">CruxEval Leaderboard</a>,
              <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">Chatbot Arena Leaderboard</a>,
              <a href="https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard">BigCode Models Leaderboard</a>,
              <a href="https://infi-coder.github.io/inficoder-eval/">InfiCoder-Eval</a>, and
              <a href="https://leaderboard.tabbyml.com/">TabbyML Leaderboard</a>.
            </p>
            <p>
              The source code from this website is borrowed from <a
                href="https://github.com/nerfies/nerfies.github.io">this template</a>!
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer> -->
</body>

</html>